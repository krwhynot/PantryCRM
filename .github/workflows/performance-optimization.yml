name: "âš¡ Performance Optimization Validation"

on:
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'src/**'
      - 'app/**'
      - 'components/**'
      - 'package.json'
      - 'next.config.js'
      - 'tsconfig.json'
  workflow_dispatch:
    inputs:
      comparison_branch:
        description: 'Branch to compare against'
        required: false
        default: 'main'
      performance_threshold:
        description: 'Performance regression threshold (%)'
        required: false
        default: '10'

permissions:
  contents: read
  issues: write
  pull-requests: write

env:
  # Azure B1 constraints
  NODE_OPTIONS: '--max-old-space-size=1536'
  NEXT_TELEMETRY_DISABLED: 1
  # Performance thresholds
  MAX_BUNDLE_SIZE_MB: 50
  MAX_BUILD_TIME_SEC: 300
  MAX_MEMORY_MB: 1536
  MIN_LIGHTHOUSE_SCORE: 70

jobs:
  # =====================================================================
  # BUNDLE SIZE ANALYSIS
  # =====================================================================
  
  bundle-analysis:
    name: "ðŸ“¦ Bundle Size Analysis"
    runs-on: ubuntu-latest
    outputs:
      current_size: ${{ steps.current.outputs.size }}
      base_size: ${{ steps.base.outputs.size }}
      size_diff: ${{ steps.compare.outputs.diff }}
      
    steps:
      - name: "ðŸ“¥ Checkout PR"
        uses: actions/checkout@v4
        
      - name: "âš™ï¸ Setup Node.js"
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          
      - name: "ðŸ“¦ Install Dependencies"
        run: npm ci --legacy-peer-deps
        
      - name: "ðŸ—ï¸ Build Current Branch"
        id: current
        run: |
          # Build with production settings
          npm run build:azure
          
          # Calculate bundle size
          BUNDLE_SIZE=$(find .next -name "*.js" -o -name "*.css" | xargs du -c | tail -1 | awk '{print $1}')
          BUNDLE_SIZE_MB=$((BUNDLE_SIZE / 1024))
          
          echo "size=$BUNDLE_SIZE_MB" >> $GITHUB_OUTPUT
          echo "Current bundle size: ${BUNDLE_SIZE_MB}MB"
          
          # Save bundle stats
          mkdir -p bundle-stats
          find .next/static -name "*.js" -exec ls -la {} \; > bundle-stats/current-files.txt
          
      - name: "ðŸ“¥ Checkout Base Branch"
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.inputs.comparison_branch || github.base_ref }}
          path: base-branch
          
      - name: "ðŸ—ï¸ Build Base Branch"
        id: base
        run: |
          cd base-branch
          npm ci --legacy-peer-deps
          npm run build:azure
          
          # Calculate bundle size
          BUNDLE_SIZE=$(find .next -name "*.js" -o -name "*.css" | xargs du -c | tail -1 | awk '{print $1}')
          BUNDLE_SIZE_MB=$((BUNDLE_SIZE / 1024))
          
          echo "size=$BUNDLE_SIZE_MB" >> $GITHUB_OUTPUT
          echo "Base bundle size: ${BUNDLE_SIZE_MB}MB"
          
      - name: "ðŸ“Š Compare Bundle Sizes"
        id: compare
        run: |
          CURRENT=${{ steps.current.outputs.size }}
          BASE=${{ steps.base.outputs.size }}
          DIFF=$((CURRENT - BASE))
          PERCENT=$((DIFF * 100 / BASE))
          
          echo "diff=$DIFF" >> $GITHUB_OUTPUT
          echo "percent=$PERCENT" >> $GITHUB_OUTPUT
          
          # Generate report
          cat > bundle-report.md << EOF
          ## ðŸ“¦ Bundle Size Analysis
          
          | Metric | Base | Current | Diff | Change |
          |--------|------|---------|------|--------|
          | Total Size | ${BASE}MB | ${CURRENT}MB | ${DIFF}MB | ${PERCENT}% |
          
          ### Largest Files
          \`\`\`
          $(find .next/static/chunks -name "*.js" -size +100k -exec ls -lh {} \; | head -10)
          \`\`\`
          EOF
          
          # Check threshold
          if [ "$CURRENT" -gt "${{ env.MAX_BUNDLE_SIZE_MB }}" ]; then
            echo "âŒ Bundle size exceeds limit (${CURRENT}MB > ${{ env.MAX_BUNDLE_SIZE_MB }}MB)" >> bundle-report.md
            exit 1
          fi
          
      - name: "ðŸ“¤ Upload Bundle Analysis"
        uses: actions/upload-artifact@v4
        with:
          name: bundle-analysis
          path: |
            bundle-stats/
            bundle-report.md
          retention-days: 7

  # =====================================================================
  # BUILD PERFORMANCE TESTING
  # =====================================================================
  
  build-performance:
    name: "ðŸ—ï¸ Build Performance"
    runs-on: ubuntu-latest
    outputs:
      build_time: ${{ steps.measure.outputs.time }}
      memory_peak: ${{ steps.measure.outputs.memory }}
      
    steps:
      - name: "ðŸ“¥ Checkout"
        uses: actions/checkout@v4
        
      - name: "âš™ï¸ Setup Node.js"
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          
      - name: "ðŸ“¦ Install Dependencies"
        run: npm ci --legacy-peer-deps
        
      - name: "ðŸ“Š Measure Build Performance"
        id: measure
        run: |
          # Monitor script
          cat > monitor.sh << 'EOF'
          #!/bin/bash
          PID=$1
          MAX_MEM=0
          while kill -0 $PID 2>/dev/null; do
            MEM=$(ps -o rss= -p $PID 2>/dev/null | awk '{print $1/1024}')
            if [ ! -z "$MEM" ] && [ $(echo "$MEM > $MAX_MEM" | bc -l) -eq 1 ]; then
              MAX_MEM=$MEM
            fi
            sleep 1
          done
          echo $MAX_MEM
          EOF
          chmod +x monitor.sh
          
          # Start build with monitoring
          START_TIME=$(date +%s)
          npm run build:azure &
          BUILD_PID=$!
          
          # Monitor memory usage
          ./monitor.sh $BUILD_PID > memory.log &
          MONITOR_PID=$!
          
          # Wait for build to complete
          wait $BUILD_PID
          BUILD_EXIT=$?
          
          # Get results
          END_TIME=$(date +%s)
          BUILD_TIME=$((END_TIME - START_TIME))
          
          # Wait for monitor to finish
          wait $MONITOR_PID
          PEAK_MEMORY=$(cat memory.log)
          
          echo "time=$BUILD_TIME" >> $GITHUB_OUTPUT
          echo "memory=$PEAK_MEMORY" >> $GITHUB_OUTPUT
          
          echo "Build completed in ${BUILD_TIME}s with peak memory ${PEAK_MEMORY}MB"
          
          # Check thresholds
          if [ "$BUILD_TIME" -gt "${{ env.MAX_BUILD_TIME_SEC }}" ]; then
            echo "âš ï¸ Build time exceeds threshold"
          fi
          
          if [ $(echo "$PEAK_MEMORY > ${{ env.MAX_MEMORY_MB }}" | bc -l) -eq 1 ]; then
            echo "âš ï¸ Memory usage exceeds Azure B1 limit"
          fi
          
          exit $BUILD_EXIT

  # =====================================================================
  # RUNTIME PERFORMANCE TESTING
  # =====================================================================
  
  runtime-performance:
    name: "ðŸš€ Runtime Performance"
    runs-on: ubuntu-latest
    needs: build-performance
    
    steps:
      - name: "ðŸ“¥ Checkout"
        uses: actions/checkout@v4
        
      - name: "âš™ï¸ Setup Node.js"
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          
      - name: "ðŸ“¦ Install Dependencies"
        run: npm ci --production --legacy-peer-deps
        
      - name: "ðŸ—ï¸ Build Application"
        run: npm run build:azure
        
      - name: "ðŸš€ Start Application"
        run: |
          npm run start &
          APP_PID=$!
          
          # Wait for app to be ready
          for i in {1..30}; do
            if curl -s http://localhost:3000/api/health > /dev/null; then
              echo "âœ… Application started"
              break
            fi
            sleep 1
          done
          
          echo "APP_PID=$APP_PID" >> $GITHUB_ENV
          
      - name: "ðŸ“Š Memory Usage Test"
        run: |
          # Initial memory
          INITIAL_MEM=$(ps -o rss= -p $APP_PID | awk '{print $1/1024}')
          echo "Initial memory: ${INITIAL_MEM}MB"
          
          # Simulate load
          for i in {1..100}; do
            curl -s http://localhost:3000/ > /dev/null &
          done
          wait
          
          # Check memory after load
          LOADED_MEM=$(ps -o rss= -p $APP_PID | awk '{print $1/1024}')
          echo "Memory after load: ${LOADED_MEM}MB"
          
          # Generate report
          cat > memory-report.md << EOF
          ## ðŸ’¾ Memory Usage Report
          
          | State | Memory (MB) |
          |-------|-------------|
          | Initial | $INITIAL_MEM |
          | After Load | $LOADED_MEM |
          | Increase | $((LOADED_MEM - INITIAL_MEM)) |
          EOF
          
      - name: "âš¡ Response Time Test"
        run: |
          # Quick performance test
          npx autocannon \
            -c 4 \
            -d 30 \
            -r 10 \
            --renderStatusCodes \
            http://localhost:3000/api/organizations \
            > performance-test.json
            
          # Parse results
          AVG_LATENCY=$(cat performance-test.json | jq '.requests.average')
          P99_LATENCY=$(cat performance-test.json | jq '.requests.p99')
          RPS=$(cat performance-test.json | jq '.requests.sent / .duration * 1000')
          
          cat >> memory-report.md << EOF
          
          ## âš¡ Response Time
          
          | Metric | Value |
          |--------|-------|
          | Avg Latency | ${AVG_LATENCY}ms |
          | P99 Latency | ${P99_LATENCY}ms |
          | Requests/sec | $RPS |
          EOF
          
      - name: "ðŸ›‘ Stop Application"
        if: always()
        run: |
          if [ ! -z "$APP_PID" ]; then
            kill $APP_PID || true
          fi
          
      - name: "ðŸ“¤ Upload Performance Report"
        uses: actions/upload-artifact@v4
        with:
          name: performance-report
          path: |
            memory-report.md
            performance-test.json
          retention-days: 7

  # =====================================================================
  # LIGHTHOUSE CI PERFORMANCE
  # =====================================================================
  
  lighthouse-performance:
    name: "ðŸ”¦ Lighthouse Performance"
    runs-on: ubuntu-latest
    
    steps:
      - name: "ðŸ“¥ Checkout"
        uses: actions/checkout@v4
        
      - name: "âš™ï¸ Setup Node.js"
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          
      - name: "ðŸ“¦ Install Dependencies"
        run: |
          npm ci --production --legacy-peer-deps
          npm install -g @lhci/cli
          
      - name: "ðŸ—ï¸ Build Application"
        run: npm run build:azure
        
      - name: "ðŸ”¦ Run Lighthouse CI"
        run: |
          # Start server
          npm run start &
          SERVER_PID=$!
          
          # Wait for server
          sleep 10
          
          # Run Lighthouse
          lhci autorun --config=.github/lighthouse/lighthouserc.json || true
          
          # Stop server
          kill $SERVER_PID || true
          
      - name: "ðŸ“Š Parse Lighthouse Results"
        run: |
          if [ -d ".lighthouseci" ]; then
            # Extract scores
            PERF_SCORE=$(find .lighthouseci -name "*.json" -exec jq '.categories.performance.score * 100' {} \; | head -1)
            
            echo "## ðŸ”¦ Lighthouse Scores" > lighthouse-report.md
            echo "Performance Score: ${PERF_SCORE}%" >> lighthouse-report.md
            
            if [ $(echo "$PERF_SCORE < ${{ env.MIN_LIGHTHOUSE_SCORE }}" | bc -l) -eq 1 ]; then
              echo "âš ï¸ Performance score below threshold" >> lighthouse-report.md
            fi
          fi
          
      - name: "ðŸ“¤ Upload Lighthouse Report"
        uses: actions/upload-artifact@v4
        with:
          name: lighthouse-report
          path: |
            .lighthouseci/
            lighthouse-report.md
          retention-days: 7

  # =====================================================================
  # PERFORMANCE SUMMARY
  # =====================================================================
  
  performance-summary:
    name: "ðŸ“Š Performance Summary"
    runs-on: ubuntu-latest
    needs: [bundle-analysis, build-performance, runtime-performance, lighthouse-performance]
    if: always()
    
    steps:
      - name: "ðŸ“¥ Download All Reports"
        uses: actions/download-artifact@v4
        with:
          path: reports/
          
      - name: "ðŸ“Š Generate Summary Report"
        run: |
          echo "# âš¡ Performance Validation Report" > performance-summary.md
          echo "" >> performance-summary.md
          echo "**Date:** $(date -u)" >> performance-summary.md
          echo "**Branch:** ${{ github.head_ref || github.ref_name }}" >> performance-summary.md
          echo "" >> performance-summary.md
          
          # Bundle Size
          if [ -f "reports/bundle-analysis/bundle-report.md" ]; then
            cat reports/bundle-analysis/bundle-report.md >> performance-summary.md
            echo "" >> performance-summary.md
          fi
          
          # Build Performance
          echo "## ðŸ—ï¸ Build Performance" >> performance-summary.md
          echo "- Build Time: ${{ needs.build-performance.outputs.build_time }}s" >> performance-summary.md
          echo "- Peak Memory: ${{ needs.build-performance.outputs.memory_peak }}MB" >> performance-summary.md
          echo "" >> performance-summary.md
          
          # Runtime Performance
          if [ -f "reports/performance-report/memory-report.md" ]; then
            cat reports/performance-report/memory-report.md >> performance-summary.md
            echo "" >> performance-summary.md
          fi
          
          # Lighthouse
          if [ -f "reports/lighthouse-report/lighthouse-report.md" ]; then
            cat reports/lighthouse-report/lighthouse-report.md >> performance-summary.md
            echo "" >> performance-summary.md
          fi
          
          # Azure B1 Compliance
          echo "## âœ… Azure B1 Compliance" >> performance-summary.md
          echo "" >> performance-summary.md
          echo "| Check | Status |" >> performance-summary.md
          echo "|-------|--------|" >> performance-summary.md
          
          # Memory check
          MEMORY_OK="âœ…"
          if [ "${{ needs.build-performance.outputs.memory_peak }}" -gt "${{ env.MAX_MEMORY_MB }}" ]; then
            MEMORY_OK="âŒ"
          fi
          echo "| Memory Usage | $MEMORY_OK |" >> performance-summary.md
          
          # Bundle size check
          BUNDLE_OK="âœ…"
          if [ "${{ needs.bundle-analysis.outputs.current_size }}" -gt "${{ env.MAX_BUNDLE_SIZE_MB }}" ]; then
            BUNDLE_OK="âŒ"
          fi
          echo "| Bundle Size | $BUNDLE_OK |" >> performance-summary.md
          
          # Build time check
          BUILD_OK="âœ…"
          if [ "${{ needs.build-performance.outputs.build_time }}" -gt "${{ env.MAX_BUILD_TIME_SEC }}" ]; then
            BUILD_OK="âŒ"
          fi
          echo "| Build Time | $BUILD_OK |" >> performance-summary.md
          
          # Copy to step summary
          cat performance-summary.md >> $GITHUB_STEP_SUMMARY
          
      - name: "ðŸ’¬ Comment on PR"
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const summary = fs.readFileSync('performance-summary.md', 'utf8');
            
            // Find and update existing comment or create new one
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const botComment = comments.find(comment => 
              comment.user.type === 'Bot' && 
              comment.body.includes('âš¡ Performance Validation Report')
            );
            
            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: summary
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: summary
              });
            }